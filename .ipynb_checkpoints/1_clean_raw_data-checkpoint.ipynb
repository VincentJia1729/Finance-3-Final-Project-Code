{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3285f0-4a37-449a-a85d-54cae6c3b212",
   "metadata": {},
   "source": [
    "# Part 1 - Clean Raw Data\n",
    "\n",
    "In this file, we clean the data mentioned in _Forecasting the Volatility of Stock Price Index: A Hybrid Model Integrating LSTM with Multiple GARCH-Type Models_ by Kim and Won. Specifically, we want to extract 6 pieces of data.  \n",
    "These are: \n",
    "- the KOSPI 200 (Korea Composite Stock Price Index of the top 200 companies by market cap)  \n",
    "- the KOSPI 200 log difference return\n",
    "- the 3-year Korean government bond interest rate\n",
    "- the 3-year Korean Corporate AA bond rate\n",
    "- the price of crude oil\n",
    "-  the price of gold.\n",
    "\n",
    "The data set is taken daily, and spans from Feb 1st 2010 to Feb 1st 2024. After cleaning, we have 3304 combined data points for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3c83e-f8ce-495c-8e88-5d096c69587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc7d1b-c845-4af4-92f5-40eb2f8ddbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel(\"./excel_data/kospi_200_index.xlsx\") # korea stock exchange index\n",
    "df2 = pd.read_excel(\"./excel_data/3_year_gov_bond_rates.xlsx\") # 3 year gov bond interest rates\n",
    "df3 = pd.read_excel(\"./excel_data/corporate_bond_spreads.xlsx\") # excess return on korean corporate bonds over government bonds\n",
    "df4 = pd.read_excel(\"./excel_data/crude_oil_prices.xlsx\") # brent crude oil prices\n",
    "df5 = pd.read_excel(\"./excel_data/gold_prices.xlsx\") # gold prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83cf334-232a-4a1f-b4d3-a0051484008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all \"date\" columns to date-time object\n",
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "df2['date'] = pd.to_datetime(df2['date'])\n",
    "df3['date'] = pd.to_datetime(df3['date'])\n",
    "df4['date'] = pd.to_datetime(df4['date'])\n",
    "df5['date'] = pd.to_datetime(df5['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a084296-7d44-4c81-9799-dee3ee8b7286",
   "metadata": {},
   "source": [
    "**Create the KOSPI 200 Index Log Return**  \n",
    "Since the most recent values are at the top, we have to reverse the dataframe / calculate log return / reverse the dataframe again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76d169-32cf-4628-88cd-f4147e8db98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the dataframe\n",
    "df1_reversed = df1.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "# calculate log return\n",
    "df1_reversed['log_return_kospi'] = np.log(df1_reversed['price'] / df1_reversed['price'].shift(1))\n",
    "\n",
    "# drop the top value of the dataframe because it is \"NaN\"\n",
    "df1_reversed = df1_reversed.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "# restore original order of the dataframe\n",
    "df1 = df1_reversed.iloc[::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea33541-0465-4340-9662-0d816e479e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 'price' columns to be unique\n",
    "df1 = df1.rename(columns={'price': 'price_kospi_raw'})\n",
    "df2 = df2.rename(columns={'price': 'interest_rate_government'})\n",
    "df3 = df3.rename(columns={'price': 'interest_rate_corporate_spread'})\n",
    "df4 = df4.rename(columns={'price': 'price_oil'})\n",
    "df5 = df5.rename(columns={'price': 'price_gold'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856210b8-3c81-49fb-8191-587ea344b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df1, df2, df3, df4, df5 \n",
    "# since df3 has the fewest rows, we will merge on df3 first\n",
    "merged_df1 = pd.merge(df3, df2, on = \"date\", how = \"inner\")\n",
    "# add corporate spread to interest rate\n",
    "merged_df1['interest_rate_corporate'] = merged_df1['interest_rate_corporate_spread'] + merged_df1['interest_rate_government']\n",
    "# drop the old column \"interest_rate_corporate_spread\"\n",
    "merged_df1.drop(columns='interest_rate_corporate_spread', inplace=True)\n",
    "\n",
    "# merge everything else\n",
    "merged_df2 = pd.merge(merged_df1, df1, on = \"date\", how = \"inner\")\n",
    "merged_df3 = pd.merge(merged_df2, df4, on = \"date\", how = \"inner\")\n",
    "merged_df4 = pd.merge(merged_df3, df5, on = \"date\", how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b018844-9fcd-4f71-b7b9-1554728a2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view data\n",
    "merged_df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347cf25-c6d7-4648-a139-dcc3881b5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-arrange column order for clarity\n",
    "df6 = merged_df4[['date', 'price_kospi_raw', 'log_return_kospi', 'interest_rate_government', \n",
    "                  'interest_rate_corporate', 'price_oil', 'price_gold' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d760a-410b-410b-b864-a163c4ad8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cdadfd-f0f2-4e70-8be3-0126442273df",
   "metadata": {},
   "source": [
    "**Calculate Realized Volatility**  \n",
    "We want to predict realized volatility with our model. To calculate realized volatility, we will use \"price_kospi_raw\". We will use a rolling window of 7 days to calculate realized volatility. We use 7 days instead of the 22 days in the paper due to hardware constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f94317-3764-43ef-a69e-dae53c2fb3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to flip the dataframe so oldest results are at the top\n",
    "df6_reversed = df6.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "# Define the maximum window size\n",
    "# we're not following the author's window size because our computer isn't as good as their's\n",
    "max_window_size = 7 \n",
    "\n",
    "# Initialize a list to store realized volatility values\n",
    "realized_volatility = []\n",
    "\n",
    "# Loop over each row in df6\n",
    "# remember that i starts from 0\n",
    "for i in range(len(df6_reversed)):\n",
    "    # Get the subset of log returns starting from the current row\n",
    "    # remaining returns is a subset of the dataframe df6_reversed\n",
    "    remaining_returns = df6_reversed['log_return_kospi'].iloc[i:] \n",
    "    \n",
    "    # Limit the number of rows to the max window size\n",
    "    if len(remaining_returns) > max_window_size:\n",
    "        # here, we truncate our data so we only get to the 7th value\n",
    "        # inclusive of the 7th value (because index starts at 0)\n",
    "        remaining_returns = remaining_returns.iloc[:max_window_size]\n",
    "    \n",
    "    # Number of days in the current window (Ï_t)\n",
    "    rho_t = len(remaining_returns)\n",
    "    \n",
    "    if rho_t > 1:  # Ensure at least two data points to compute variance\n",
    "        # Calculate the mean of the remaining returns\n",
    "        mean_return = remaining_returns.mean()\n",
    "        \n",
    "        # Calculate realized volatility (standard deviation of mean-centered returns)\n",
    "        rv_t = np.sqrt((1 / rho_t) * np.sum((remaining_returns - mean_return) ** 2))\n",
    "    else:\n",
    "        rv_t = np.nan  # Not enough data to compute volatility\n",
    "    \n",
    "    realized_volatility.append(rv_t)\n",
    "\n",
    "# Use .loc to avoid SettingWithCopyWarning\n",
    "df6_reversed.loc[:, 'realized_volatility'] = realized_volatility\n",
    "\n",
    "# reverse the dataframe so newest results are on the top\n",
    "df6 = df6_reversed.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "# drop the top value because it is a \"NaN\"\n",
    "df6 = df6.drop(index=0).reset_index(drop=True)\n",
    "# drop all rows with \"NaN\"\n",
    "df6 = df6.dropna()\n",
    "df6\n",
    "\n",
    "# now our data goes from 2010/02/02 to 2024/01/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9804fc7-e657-4fbd-82d3-0fcf4feb43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets graph the realized volatility\n",
    "plt.plot(df6_reversed['date'], df6_reversed['realized_volatility'], color='blue')\n",
    "plt.xlabel(\"Time in Days\")\n",
    "plt.ylabel(\"Realized Volatility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45fd73-b617-4d2d-9f46-5997d8d36ea8",
   "metadata": {},
   "source": [
    "We can see that the realized volatility is consistent with the real world. The spike in 2020 coincides with Covid-19."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543a0c5-726f-496c-a2be-e1de359011dd",
   "metadata": {},
   "source": [
    "**Split into Training and Testing Data Sets**  \n",
    "We will follow the convention from the paper and use 66% of the data to train and 34% of the data to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb91689-8f40-44d6-abec-1d47cce0ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = df6.shape[0]\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5158c-9f5a-40a4-90cf-a73bd4be0600",
   "metadata": {},
   "source": [
    "We can see that we have 3304 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20755bb2-42e6-401e-a17f-ee816d3d53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.34 * num_rows) \n",
    "row_34 = df6.iloc[split_index]\n",
    "print(row_34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd9fd9-5564-4647-89ad-7260fa5465f7",
   "metadata": {},
   "source": [
    "We see that we will split the data on 2019/05/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f092b75-1a46-43ab-99fd-af621ba70943",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_data =  df6.iloc[:split_index] # data from 2019/05/20 to 2024/01/31 (only trading days in dataset)\n",
    "df_training_data = df6.iloc[split_index:] # data from 2010/02/02 to 2019/05/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bcb33c-9e0e-4020-83de-b410e2616f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put oldest data on top\n",
    "df_testing_data = df_testing_data.iloc[::-1].reset_index(drop=True)\n",
    "df_training_data = df_training_data.iloc[::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8492673-3037-45e0-ab4e-aee239af62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99258fdf-6919-4a62-a8d8-d2cdd0890479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a7451-1e5d-4495-8ea2-b8ff826056e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7362102-ae11-4c32-8d28-107f6e76d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa20dddd-b0e3-48bc-9827-c3ce5b9b01e9",
   "metadata": {},
   "source": [
    "**Fit GARCH, EGARCH, EWMA Parameters**  \n",
    "We will fit the GARCH, EGARCH, and EWMA parameters over the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80858b-941a-41fe-abc6-c04bc9c20b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch11 = arch_model(df_training_data['log_return_kospi'], vol='GARCH', p=1, q=1, rescale = False)\n",
    "fitted_model = garch11.fit()\n",
    "print(fitted_model.summary())\n",
    "# omega = 1.9716e-06 = 0.0000019716\n",
    "# alpha = 0.0500\n",
    "# beta = 0.9300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d37eb6-2fa5-4b00-b448-bc864d11acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGARCH Model\n",
    "egarch11 = arch_model(df_training_data['log_return_kospi'], vol='EGARCH',  p=1, q=1, mean='Constant', dist='Normal',rescale = False)\n",
    "fitted_modelv2 = egarch11.fit()\n",
    "print(fitted_modelv2.summary())\n",
    "# omega = -0.1462\n",
    "# alpha = 0.1077\n",
    "# beta = 0.9839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344e68c-1b38-4889-a5bb-af164fe8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal EWMA parameters\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "ewma_model = SimpleExpSmoothing(df_training_data['log_return_kospi']).fit()\n",
    "optimal_alpha = ewma_model.model.params['smoothing_level']\n",
    "# we can think of alpha as 1 - rho\n",
    "print(\"rho = \", 1 - optimal_alpha)\n",
    "# thus we know the rho = lambda parameter as defined by the paper\n",
    "# rho = 0.985 Round for convenience\n",
    "# 1 - rho = 0.015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe87ef6-c2aa-466b-8d4c-09e76e5855fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parameters into the dataframes\n",
    "df_training_data = df_training_data.assign (\n",
    "    omega_garch = 0.0000019716, \n",
    "    alpha_garch = 0.0500,\n",
    "    beta_garch = 0.9300, \n",
    "    omega_egarch = -0.1462, \n",
    "    alpha_egarch = 0.1077, \n",
    "    beta_egarch = 0.9839, \n",
    "    rho_ewma = 0.985, \n",
    "    one_minus_rho_ewma = 0.015\n",
    ")\n",
    "df_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3819b4-ebc7-4ccc-91c7-d4813e8545ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_data = df_testing_data.assign (\n",
    "    omega_garch = 0.0000019716, \n",
    "    alpha_garch = 0.0500,\n",
    "    beta_garch = 0.9300, \n",
    "    omega_egarch = -0.1462, \n",
    "    alpha_egarch = 0.1077, \n",
    "    beta_egarch = 0.9839, \n",
    "    rho_ewma = 0.985, \n",
    "    one_minus_rho_ewma = 0.015\n",
    ")\n",
    "df_testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc352cd-5273-4b93-94cd-f8a79b2cfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_data.to_excel(\"./excel_data/testing_data.xlsx\", index=False)\n",
    "df_training_data.to_excel(\"./excel_data/training_data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c869789-e5b5-4226-b223-2abab3067c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02cfea3-9899-44ec-99d4-e3becd25d029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
