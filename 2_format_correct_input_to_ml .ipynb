{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffb4bed-5985-45cf-b067-932f39c334af",
   "metadata": {},
   "source": [
    "# 2 Format Correct Input to ML \n",
    "\n",
    "The most difficult and tedious part of this project is formatting our data into the correct \"shape\" so our machine learning program can read the data. We have to manually format our training and testing data. We will save our training data as a numpy array.  \n",
    "In their paper, the authors suggested creating (2 * 2 * 8) - 14 = 18 data sets. \n",
    "The breakdown is as follows. We will need to create training / testing data sets. For each training / testing data set, we will need to create \"input parameters (x)\" and \"output parameters (y)\". In total, there are 8 ways to group the data sets. \n",
    "\n",
    "1. Explanatory Variables\n",
    "2. Explanatory Variables + Garch Params\n",
    "3. Explanatory Variables + Egarch Params\n",
    "4. Explanatory Variables + Ewma Params\n",
    "5. Explanatory Variables + Garch Params + Egarch Params\n",
    "6. Explanatory Variables + Garch Params + Ewma Params\n",
    "7. Explanatory Variables + Egarch Params + Ewma Params\n",
    "8. Explanatory Variables + Garch Params + Egarch Params + Ewma Params\n",
    "\n",
    "The reason we subtract 14 is because the training_y and testing_y are the same for groups 1-8. \n",
    "\n",
    "However, creating all of the data sets would be incredibly tedious. For the sake of time, we will create data set 8 (Explanatory Variables + Garch Params + Egarch Params + Ewma Params), and data set 1 (Explanatory Variables) only. In total, we will create 6 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e2f57c50-4d36-46eb-bc0f-2e19caef6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecf147a-4eee-4e48-8ca9-ceb8d8e7ba77",
   "metadata": {},
   "source": [
    "**Creating Data Set 8**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9facd10-dc3d-478b-a7a7-66dea16cf408",
   "metadata": {},
   "source": [
    "**Training Set** - data_8_training_x, training_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9bf93e84-dbe1-4a82-a5da-fd2b25394878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_excel(\"./excel_data/training_data.xlsx\") # training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b18c59b5-19d3-4101-92dd-1fdd553dedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431eb4e-9964-4250-be9f-54fab3488662",
   "metadata": {},
   "source": [
    "Work on x data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c28caba1-69a9-4b52-addf-27ec362f0c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of subsets_x_training_numpy is:  (2174, 7, 14)\n"
     ]
    }
   ],
   "source": [
    "x_training = df_training.drop(columns = ['date','realized_volatility'])\n",
    "x_training_numpy = x_training.to_numpy() \n",
    "subsets_x_training = [x_training_numpy[i:i+window_size] for i in range(len(x_training_numpy) - window_size)]\n",
    "subsets_x_training_numpy = np.array(subsets_x_training)\n",
    "print(\"The shape of subsets_x_training_numpy is: \", subsets_x_training_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "96f7e9cf-5821-484c-aac6-e758867e880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2.09270000e+02 -7.09476109e-03  4.28000000e+00  5.33000000e+00\n",
      "    7.72300000e+01  1.11445000e+03  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [ 2.11860000e+02  1.23003949e-02  4.27000000e+00  5.32000000e+00\n",
      "    7.69800000e+01  1.10980000e+03  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [ 2.11720000e+02 -6.61032179e-04  4.28000000e+00  5.32000000e+00\n",
      "    7.31400000e+01  1.06353000e+03  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [ 2.05060000e+02 -3.19620277e-02  4.22500000e+00  5.26500000e+00\n",
      "    7.11900000e+01  1.06625000e+03  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [ 2.03370000e+02 -8.27563913e-03  4.23000000e+00  5.26000000e+00\n",
      "    7.18900000e+01  1.06285000e+03  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [ 2.06010000e+02  1.28977312e-02  4.24000000e+00  5.26000000e+00\n",
      "    7.37500000e+01  1.07810000e+03  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [ 2.05940000e+02 -3.39847072e-04  4.23000000e+00  5.24000000e+00\n",
      "    7.45200000e+01  1.07210000e+03  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]]]\n"
     ]
    }
   ],
   "source": [
    "head = subsets_x_training_numpy[:1]\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f75d9f-819d-4f51-8a8c-5391ed94e02b",
   "metadata": {},
   "source": [
    "To make the training converge faster, we will normalize the first 6 features (to mean = 0 and standard deviation = 1) while keeping the last 8 features the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e167974e-998a-4237-9166-28ee9ebc410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first 6 features and last 8 features\n",
    "first_6_features = subsets_x_training_numpy[:, :, :6]\n",
    "last_8_features = subsets_x_training_numpy[:, :, 6:]\n",
    "\n",
    "# Compute mean and std for the first 6 features across samples and timesteps\n",
    "# this normalizes across each of the first 6 features\n",
    "mean = np.mean(first_6_features, axis=(0, 1), keepdims=True)\n",
    "std = np.std(first_6_features, axis=(0, 1), keepdims=True)\n",
    "\n",
    "# Normalize the first 6 features\n",
    "normalized_first_6 = (first_6_features - mean) / std\n",
    "\n",
    "# Concatenate normalized and unchanged features\n",
    "data_8_training_x = np.concatenate([normalized_first_6, last_8_features], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "81d6f802-e1e5-46e6-b851-2d3ff1ddbfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized Data:\n",
      "[[[-2.02679181e+00 -7.24829151e-01  2.24219359e+00  2.51619232e+00\n",
      "    1.61261941e-01 -1.26125674e+00  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [-1.92910201e+00  1.23038370e+00  2.22953566e+00  2.50535096e+00\n",
      "    1.50046136e-01 -1.28705881e+00  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [-1.93438254e+00 -7.62491951e-02  2.24219359e+00  2.50535096e+00\n",
      "   -2.22286390e-02 -1.54380323e+00  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [-2.18558488e+00 -3.23168185e+00  2.17257499e+00  2.44572348e+00\n",
      "   -1.09711923e-01 -1.52871041e+00  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [-2.24932842e+00 -8.43872685e-01  2.17890395e+00  2.44030280e+00\n",
      "   -7.83076672e-02 -1.54757644e+00  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [-2.14975271e+00  1.29060077e+00  2.19156188e+00  2.44030280e+00\n",
      "    5.13792681e-03 -1.46295676e+00  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]\n",
      "  [-2.15239298e+00 -4.38707369e-02  2.17890395e+00  2.41862008e+00\n",
      "    3.96826082e-02 -1.49624975e+00  1.97160000e-06  5.00000000e-02\n",
      "    9.30000000e-01 -1.46200000e-01  1.07700000e-01  9.83900000e-01\n",
      "    9.85000000e-01  1.50000000e-02]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNormalized Data:\")\n",
    "head = data_8_training_x[:1] # results look correct\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f0f65-60b9-47d3-9c4f-01759c710e3b",
   "metadata": {},
   "source": [
    "Now, let's work on the y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "16079af0-6c23-4af7-8748-f7b5fefebc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training = df_training[['realized_volatility']]\n",
    "y_training_numpy = y_training['realized_volatility'].to_numpy()\n",
    "y_training_truncated = y_training_numpy[window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "59ce72a0-1cde-49b9-9972-518bde99aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy data to folder\n",
    "folder_path = \"./numpy_data\"\n",
    "os.makedirs(folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "76d48898-6710-4a62-a0d9-d5b31ef991b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_8_training_x\n",
    "file_path = os.path.join(folder_path, \"data_8_training_x.npy\")\n",
    "np.save(file_path, data_8_training_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0b499921-42a0-4714-9b82-dc97b6a568ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save y_training_truncated\n",
    "file_path = os.path.join(folder_path, \"training_y.npy\")\n",
    "np.save(file_path, y_training_truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df8b0c-08d6-4be9-8ffc-afed9f20d7d1",
   "metadata": {},
   "source": [
    "**Now work on the testing set** - data_8_testing_x, testing_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f1b307c9-c6b6-4c22-b6bc-930830aa80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = pd.read_excel(\"./excel_data/testing_data.xlsx\") # training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "29a4310c-28f2-4fb1-89af-29afd85b462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of subsets_x_testing_numpy is:  (1116, 7, 14)\n"
     ]
    }
   ],
   "source": [
    "x_testing = df_testing.drop(columns = ['date','realized_volatility'])\n",
    "x_testing_numpy = x_testing.to_numpy() \n",
    "subsets_x_testing = [x_testing_numpy[i:i+window_size] for i in range(len(x_testing_numpy) - window_size)]\n",
    "subsets_x_testing_numpy = np.array(subsets_x_testing)\n",
    "print(\"The shape of subsets_x_testing_numpy is: \", subsets_x_testing_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f214ceb9-36d4-4e3a-bfc2-f3092ea6607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first 6 features and last 8 features\n",
    "first_6_features = subsets_x_testing_numpy[:, :, :6]\n",
    "last_8_features = subsets_x_testing_numpy[:, :, 6:]\n",
    "\n",
    "# Compute mean and std for the first 6 features across samples and timesteps\n",
    "# this normalizes across each of the first 6 features\n",
    "mean = np.mean(first_6_features, axis=(0, 1), keepdims=True)\n",
    "std = np.std(first_6_features, axis=(0, 1), keepdims=True)\n",
    "\n",
    "# Normalize the first 6 features\n",
    "normalized_first_6 = (first_6_features - mean) / std\n",
    "\n",
    "# Concatenate normalized and unchanged features\n",
    "data_8_testing_x = np.concatenate([normalized_first_6, last_8_features], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "71e6730b-4812-4b1f-af8e-f8c1c3f73678",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_testing = df_testing[['realized_volatility']]\n",
    "y_testing_numpy = y_testing['realized_volatility'].to_numpy()\n",
    "y_testing_truncated = y_testing_numpy[window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "43879347-79a3-42d7-ba0b-82d320519dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1116,)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_testing_truncated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "463e01fc-7bf2-4291-9eb0-853121555e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_8_testing_x\n",
    "file_path = os.path.join(folder_path, \"data_8_testing_x.npy\")\n",
    "np.save(file_path, data_8_testing_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a0ed9-9ac6-45bd-ba9f-af59a8460f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save y_testing_truncated\n",
    "file_path = os.path.join(folder_path, \"testing_y.npy\")\n",
    "np.save(file_path, y_testing_truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6189c21-4112-4c05-a8b8-28bceccd0f05",
   "metadata": {},
   "source": [
    "**Creating Data Set 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a434df-08d7-42ea-bfeb-ff954ebed0f0",
   "metadata": {},
   "source": [
    "Make sure to save data_1_training_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "15b4f5b5-4261-4d15-8536-ac502e197110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_kospi_raw</th>\n",
       "      <th>log_return_kospi</th>\n",
       "      <th>interest_rate_government</th>\n",
       "      <th>interest_rate_corporate</th>\n",
       "      <th>price_oil</th>\n",
       "      <th>price_gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>209.27</td>\n",
       "      <td>-0.007095</td>\n",
       "      <td>4.280</td>\n",
       "      <td>5.330</td>\n",
       "      <td>77.23</td>\n",
       "      <td>1114.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211.86</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>4.270</td>\n",
       "      <td>5.320</td>\n",
       "      <td>76.98</td>\n",
       "      <td>1109.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211.72</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>4.280</td>\n",
       "      <td>5.320</td>\n",
       "      <td>73.14</td>\n",
       "      <td>1063.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205.06</td>\n",
       "      <td>-0.031962</td>\n",
       "      <td>4.225</td>\n",
       "      <td>5.265</td>\n",
       "      <td>71.19</td>\n",
       "      <td>1066.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>203.37</td>\n",
       "      <td>-0.008276</td>\n",
       "      <td>4.230</td>\n",
       "      <td>5.260</td>\n",
       "      <td>71.89</td>\n",
       "      <td>1062.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price_kospi_raw  log_return_kospi  interest_rate_government  \\\n",
       "0           209.27         -0.007095                     4.280   \n",
       "1           211.86          0.012300                     4.270   \n",
       "2           211.72         -0.000661                     4.280   \n",
       "3           205.06         -0.031962                     4.225   \n",
       "4           203.37         -0.008276                     4.230   \n",
       "\n",
       "   interest_rate_corporate  price_oil  price_gold  \n",
       "0                    5.330      77.23     1114.45  \n",
       "1                    5.320      76.98     1109.80  \n",
       "2                    5.320      73.14     1063.53  \n",
       "3                    5.265      71.19     1066.25  \n",
       "4                    5.260      71.89     1062.85  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training = df_training.drop(columns = ['date','realized_volatility'])\n",
    "x_training = x_training.drop(x_training.columns[-8:], axis=1)\n",
    "x_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e066d-90c5-4a0e-a164-e3dc4b07c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training_numpy = x_training.to_numpy() \n",
    "subsets_x_training = [x_training_numpy[i:i+window_size] for i in range(len(x_training_numpy) - window_size)]\n",
    "subsets_x_training_numpy = np.array(subsets_x_training)\n",
    "print(\"The shape of subsets_x_training_numpy is: \", subsets_x_training_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9990ec6d-c0b6-4748-9c5c-8af678371704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2.09270000e+02 -7.09476109e-03  4.28000000e+00  5.33000000e+00\n",
      "    7.72300000e+01  1.11445000e+03]\n",
      "  [ 2.11860000e+02  1.23003949e-02  4.27000000e+00  5.32000000e+00\n",
      "    7.69800000e+01  1.10980000e+03]\n",
      "  [ 2.11720000e+02 -6.61032179e-04  4.28000000e+00  5.32000000e+00\n",
      "    7.31400000e+01  1.06353000e+03]\n",
      "  [ 2.05060000e+02 -3.19620277e-02  4.22500000e+00  5.26500000e+00\n",
      "    7.11900000e+01  1.06625000e+03]\n",
      "  [ 2.03370000e+02 -8.27563913e-03  4.23000000e+00  5.26000000e+00\n",
      "    7.18900000e+01  1.06285000e+03]\n",
      "  [ 2.06010000e+02  1.28977312e-02  4.24000000e+00  5.26000000e+00\n",
      "    7.37500000e+01  1.07810000e+03]\n",
      "  [ 2.05940000e+02 -3.39847072e-04  4.23000000e+00  5.24000000e+00\n",
      "    7.45200000e+01  1.07210000e+03]]]\n"
     ]
    }
   ],
   "source": [
    "head = subsets_x_training_numpy[:1]\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f33c5363-866d-4f90-aa2d-ccf45974d290",
   "metadata": {},
   "outputs": [],
   "source": [
    " first_6_features = subsets_x_training_numpy[:, :, :6]\n",
    "\n",
    "# Compute mean and std for the first 6 features across samples and timesteps\n",
    "# this normalizes across each of the first 6 features\n",
    "mean = np.mean(first_6_features, axis=(0, 1), keepdims=True)\n",
    "std = np.std(first_6_features, axis=(0, 1), keepdims=True)\n",
    "\n",
    "# Normalize the first 6 features\n",
    "data_1_training_x = (first_6_features - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "58f2f917-7322-41ce-9835-bb5da459c3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized Data:\n",
      "[[[-2.02679181 -0.72482915  2.24219359  2.51619232  0.16126194\n",
      "   -1.26125674]\n",
      "  [-1.92910201  1.2303837   2.22953566  2.50535096  0.15004614\n",
      "   -1.28705881]\n",
      "  [-1.93438254 -0.0762492   2.24219359  2.50535096 -0.02222864\n",
      "   -1.54380323]\n",
      "  [-2.18558488 -3.23168185  2.17257499  2.44572348 -0.10971192\n",
      "   -1.52871041]\n",
      "  [-2.24932842 -0.84387268  2.17890395  2.4403028  -0.07830767\n",
      "   -1.54757644]\n",
      "  [-2.14975271  1.29060077  2.19156188  2.4403028   0.00513793\n",
      "   -1.46295676]\n",
      "  [-2.15239298 -0.04387074  2.17890395  2.41862008  0.03968261\n",
      "   -1.49624975]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNormalized Data:\")\n",
    "head = data_1_training_x[:1] # results look correct\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "97e405c5-f11a-4e27-8ceb-d67ec91dd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(folder_path, \"data_1_training_x.npy\")\n",
    "np.save(file_path, data_1_training_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044c14c-52dd-486a-961e-e90ee102a3c9",
   "metadata": {},
   "source": [
    "Make sure to save data_1_testing_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "ff3a963a-acf2-4646-9c28-b595909826d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of subsets_x_testing_numpy is:  (1116, 7, 6)\n"
     ]
    }
   ],
   "source": [
    "df_testing = pd.read_excel(\"./excel_data/testing_data.xlsx\") # training dataframe\n",
    "x_testing = df_testing.drop(columns = ['date','realized_volatility'])\n",
    "x_testing = x_testing.drop(x_testing.columns[-8:], axis=1)\n",
    "x_testing_numpy = x_testing.to_numpy() \n",
    "subsets_x_testing = [x_testing_numpy[i:i+window_size] for i in range(len(x_testing_numpy) - window_size)]\n",
    "subsets_x_testing_numpy = np.array(subsets_x_testing)\n",
    "print(\"The shape of subsets_x_testing_numpy is: \", subsets_x_testing_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "02b66a7d-851e-47cf-abeb-9bc72e76203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "first_6_features = subsets_x_testing_numpy[:, :, :6]\n",
    "\n",
    "# Compute mean and std for the first 6 features across samples and timesteps\n",
    "# this normalizes across each of the first 6 features\n",
    "mean = np.mean(first_6_features, axis=(0, 1), keepdims=True)\n",
    "std = np.std(first_6_features, axis=(0, 1), keepdims=True)\n",
    "\n",
    "# Normalize the first 6 features\n",
    "data_1_testing_x = (first_6_features - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "3c547667-7935-43b8-850f-d55ca6c01976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.31056959  0.17027519 -0.43312125 -0.62131363 -0.23435912\n",
      "   -2.98016009]\n",
      "  [-1.28844425  0.33212355 -0.45056801 -0.63592424 -0.23959993\n",
      "   -2.99870023]\n",
      "  [-1.27833525  0.14554695 -0.44533398 -0.63884636 -0.31440065\n",
      "   -3.00661226]\n",
      "  [-1.28348512 -0.08995626 -0.46278073 -0.65345698 -0.48163029\n",
      "   -2.94691776]\n",
      "  [-1.32086932 -0.58972175 -0.46714242 -0.65710963 -0.44732678\n",
      "   -2.9381791 ]\n",
      "  [-1.31552872  0.07844127 -0.45492969 -0.6468822  -0.42302845\n",
      "   -2.9712443 ]\n",
      "  [-1.37904373 -1.00344307 -0.48720619 -0.67391183 -0.4387509\n",
      "   -2.96858727]]]\n"
     ]
    }
   ],
   "source": [
    "head = data_1_testing_x[:1] # results look correct\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e9c762d3-6d15-43e8-86d7-010a7a22e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_1_testing_x\n",
    "file_path = os.path.join(folder_path, \"data_1_testing_x.npy\")\n",
    "np.save(file_path, data_1_testing_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdae79c-18f5-45ec-8453-0d01bc111c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
